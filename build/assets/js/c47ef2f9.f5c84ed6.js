"use strict";(globalThis.webpackChunkphysical_ai_docs=globalThis.webpackChunkphysical_ai_docs||[]).push([[745],{6917(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"physical-ai/module-1-ros2/chapter-3-code-to-body","title":"Chapter 3 - From Code to Body","description":"Introduction","source":"@site/docs/physical-ai/module-1-ros2/chapter-3-code-to-body.md","sourceDirName":"physical-ai/module-1-ros2","slug":"/physical-ai/module-1-ros2/chapter-3-code-to-body","permalink":"/docs/physical-ai/module-1-ros2/chapter-3-code-to-body","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/module-1-ros2/chapter-3-code-to-body.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 3 - From Code to Body","sidebar_label":"Chapter 3: From Code to Body"},"sidebar":"physicalAISidebar","previous":{"title":"Chapter 2: Communication in Motion","permalink":"/docs/physical-ai/module-1-ros2/chapter-2-communication-motion"},"next":{"title":"Chapter 1: The Digital Twin \u2014 Simulating Reality","permalink":"/docs/physical-ai/module-2-digital-twin/chapter-1-digital-twin-reality"}}');var o=i(4848),r=i(8453);const s={title:"Chapter 3 - From Code to Body",sidebar_label:"Chapter 3: From Code to Body"},a="Chapter 3: From Code to Body \u2014 Python Agents, rclpy, and URDF",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Bridging Python AI Agents to ROS 2 with rclpy",id:"bridging-python-ai-agents-to-ros-2-with-rclpy",level:2},{value:"What is rclpy?",id:"what-is-rclpy",level:3},{value:"Installing rclpy",id:"installing-rclpy",level:3},{value:"Creating Your First Python Node",id:"creating-your-first-python-node",level:3},{value:"Advanced AI Integration Patterns",id:"advanced-ai-integration-patterns",level:3},{value:"Pattern 1: Perception Pipeline",id:"pattern-1-perception-pipeline",level:4},{value:"Pattern 2: Behavior Trees",id:"pattern-2-behavior-trees",level:4},{value:"Understanding URDF: Links, Joints, and Frames",id:"understanding-urdf-links-joints-and-frames",level:2},{value:"Links: The Building Blocks",id:"links-the-building-blocks",level:3},{value:"Joints: Connecting Links",id:"joints-connecting-links",level:3},{value:"Frames: Coordinate Systems",id:"frames-coordinate-systems",level:3},{value:"Modeling Humanoid Robots",id:"modeling-humanoid-robots",level:3},{value:"How URDF Enables Simulation and Real-World Deployment",id:"how-urdf-enables-simulation-and-real-world-deployment",level:2},{value:"Simulation Benefits",id:"simulation-benefits",level:3},{value:"Real-World Deployment",id:"real-world-deployment",level:3},{value:"Preparing Robot Description for Later Simulation Modules",id:"preparing-robot-description-for-later-simulation-modules",level:2},{value:"1. Proper Inertial Properties",id:"1-proper-inertial-properties",level:3},{value:"2. Collision and Visual Elements",id:"2-collision-and-visual-elements",level:3},{value:"3. Transmission Elements for Actuators",id:"3-transmission-elements-for-actuators",level:3},{value:"Integration Example: Python AI Agent with URDF Robot",id:"integration-example-python-ai-agent-with-urdf-robot",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-from-code-to-body--python-agents-rclpy-and-urdf",children:"Chapter 3: From Code to Body \u2014 Python Agents, rclpy, and URDF"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"In the previous chapters, we explored ROS 2 as the robotic nervous system and learned about its communication patterns. Now, we'll bridge the gap between Python AI agents and physical robot components using ROS 2's Python client library (rclpy) and the Unified Robot Description Format (URDF). This chapter will show you how to connect your Python code to the physical world of robotics."}),"\n",(0,o.jsx)(n.h2,{id:"bridging-python-ai-agents-to-ros-2-with-rclpy",children:"Bridging Python AI Agents to ROS 2 with rclpy"}),"\n",(0,o.jsx)(n.p,{children:"Python is one of the most popular languages for AI development, with rich libraries for machine learning, computer vision, and data processing. The rclpy library allows Python programs to integrate seamlessly with ROS 2 systems, enabling AI agents to interact with robot hardware."}),"\n",(0,o.jsx)(n.h3,{id:"what-is-rclpy",children:"What is rclpy?"}),"\n",(0,o.jsx)(n.p,{children:"rclpy is the Python client library for ROS 2. It provides Python bindings for ROS 2 concepts like nodes, publishers, subscribers, services, and actions. With rclpy, you can create ROS 2 nodes in Python that communicate with nodes written in other languages like C++."}),"\n",(0,o.jsx)(n.h3,{id:"installing-rclpy",children:"Installing rclpy"}),"\n",(0,o.jsx)(n.p,{children:"rclpy is typically installed as part of the ROS 2 Python development packages. You can install it in your Python environment:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install rclpy\n"})}),"\n",(0,o.jsx)(n.h3,{id:"creating-your-first-python-node",children:"Creating Your First Python Node"}),"\n",(0,o.jsx)(n.p,{children:"Let's create a simple Python node that acts as an AI agent:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan\nimport numpy as np\n\nclass AIAgentNode(Node):\n    def __init__(self):\n        super().__init__('ai_agent_node')\n\n        # Create a subscriber to receive sensor data\n        self.subscription = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.laser_callback,\n            10\n        )\n\n        # Create a publisher to send commands\n        self.publisher = self.create_publisher(\n            String,\n            'ai_commands',\n            10\n        )\n\n        self.get_logger().info('AI Agent Node initialized')\n\n    def laser_callback(self, msg):\n        # Process sensor data with AI logic\n        ranges = np.array(msg.ranges)\n        # Remove invalid readings\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Simple AI: if something is close in front, send a stop command\n        if len(valid_ranges) > 0 and np.min(valid_ranges) < 1.0:  # Less than 1 meter\n            cmd_msg = String()\n            cmd_msg.data = 'STOP'\n            self.publisher.publish(cmd_msg)\n            self.get_logger().info('Obstacle detected, STOP command sent')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_agent = AIAgentNode()\n\n    try:\n        rclpy.spin(ai_agent)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_agent.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"advanced-ai-integration-patterns",children:"Advanced AI Integration Patterns"}),"\n",(0,o.jsx)(n.h4,{id:"pattern-1-perception-pipeline",children:"Pattern 1: Perception Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n        self.bridge = CvBridge()\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Apply AI perception algorithms\n        processed_image = self.detect_objects(cv_image)\n\n        # Publish results\n        # (Additional code to publish processed results)\n\n    def detect_objects(self, image):\n        # Placeholder for AI object detection\n        # In practice, you'd use models like YOLO, SSD, etc.\n        return image\n"})}),"\n",(0,o.jsx)(n.h4,{id:"pattern-2-behavior-trees",children:"Pattern 2: Behavior Trees"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport py_trees\n\nclass BehaviorTreeAgent(Node):\n    def __init__(self):\n        super().__init__('behavior_tree_agent')\n\n        # Create behavior tree\n        self.root = self.create_behavior_tree()\n\n        # Timer to tick the tree\n        self.timer = self.create_timer(0.1, self.tick_tree)\n\n    def create_behavior_tree(self):\n        # Create a simple patrol behavior\n        root = py_trees.composites.Sequence('Patrol')\n        check_battery = py_trees.behaviours.CheckBattery('CheckBattery')\n        navigate = py_trees.behaviours.Navigate('Navigate')\n\n        root.add_children([check_battery, navigate])\n        return root\n\n    def tick_tree(self):\n        self.root.tick_once()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"understanding-urdf-links-joints-and-frames",children:"Understanding URDF: Links, Joints, and Frames"}),"\n",(0,o.jsx)(n.p,{children:"URDF (Unified Robot Description Format) is an XML-based format used to describe robot models in ROS. It defines the physical structure of a robot including links, joints, and their relationships."}),"\n",(0,o.jsx)(n.h3,{id:"links-the-building-blocks",children:"Links: The Building Blocks"}),"\n",(0,o.jsx)(n.p,{children:"A link represents a rigid body in the robot. Each link has:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Physical properties (mass, inertia, visual appearance)"}),"\n",(0,o.jsx)(n.li,{children:"Collision properties"}),"\n",(0,o.jsx)(n.li,{children:"A coordinate frame"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Example of a simple link:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<link name="base_link">\n  <inertial>\n    <mass value="1.0"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n  </inertial>\n  <visual>\n    <origin xyz="0 0 0"/>\n    <geometry>\n      <box size="0.5 0.5 0.2"/>\n    </geometry>\n  </visual>\n  <collision>\n    <origin xyz="0 0 0"/>\n    <geometry>\n      <box size="0.5 0.5 0.2"/>\n    </geometry>\n  </collision>\n</link>\n'})}),"\n",(0,o.jsx)(n.h3,{id:"joints-connecting-links",children:"Joints: Connecting Links"}),"\n",(0,o.jsx)(n.p,{children:"Joints define the kinematic and dynamic relationships between links. Types of joints include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Fixed: No movement"}),"\n",(0,o.jsx)(n.li,{children:"Revolute: Rotational movement around an axis"}),"\n",(0,o.jsx)(n.li,{children:"Continuous: Rotational movement without limits"}),"\n",(0,o.jsx)(n.li,{children:"Prismatic: Linear movement along an axis"}),"\n",(0,o.jsx)(n.li,{children:"Floating: 6 degrees of freedom"}),"\n",(0,o.jsx)(n.li,{children:"Planar: Movement in a plane"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Example of a joint:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<joint name="wheel_joint" type="continuous">\n  <parent link="base_link"/>\n  <child link="wheel_link"/>\n  <origin xyz="0.2 0 0" rpy="0 0 0"/>\n  <axis xyz="0 1 0"/>\n</joint>\n'})}),"\n",(0,o.jsx)(n.h3,{id:"frames-coordinate-systems",children:"Frames: Coordinate Systems"}),"\n",(0,o.jsx)(n.p,{children:"Each link has its own coordinate frame. TF (Transform) trees allow ROS to understand the spatial relationships between all frames in the robot and environment."}),"\n",(0,o.jsx)(n.h3,{id:"modeling-humanoid-robots",children:"Modeling Humanoid Robots"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots require special considerations in URDF:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="simple_humanoid">\n  \x3c!-- Torso --\x3e\n  <link name="torso">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>\n      </geometry>\n    </visual>\n  </link>\n\n  \x3c!-- Head --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n    </visual>\n  </link>\n\n  <joint name="neck_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.3"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Left Arm --\x3e\n  <link name="left_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.3" radius="0.05"/>\n      </geometry>\n    </visual>\n  </link>\n\n  <joint name="left_shoulder_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="left_upper_arm"/>\n    <origin xyz="0.15 0.1 0.2"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Additional links and joints for arms, legs, etc. --\x3e\n</robot>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"how-urdf-enables-simulation-and-real-world-deployment",children:"How URDF Enables Simulation and Real-World Deployment"}),"\n",(0,o.jsx)(n.h3,{id:"simulation-benefits",children:"Simulation Benefits"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Physics simulation with accurate dynamics"}),"\n",(0,o.jsx)(n.li,{children:"Sensor simulation (camera, LIDAR, IMU)"}),"\n",(0,o.jsx)(n.li,{children:"Collision detection"}),"\n",(0,o.jsx)(n.li,{children:"Visualization in tools like RViz and Gazebo"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"real-world-deployment",children:"Real-World Deployment"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Robot state publishing (robot_state_publisher)"}),"\n",(0,o.jsx)(n.li,{children:"Forward and inverse kinematics"}),"\n",(0,o.jsx)(n.li,{children:"Motion planning compatibility"}),"\n",(0,o.jsx)(n.li,{children:"Hardware interface mapping"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"preparing-robot-description-for-later-simulation-modules",children:"Preparing Robot Description for Later Simulation Modules"}),"\n",(0,o.jsx)(n.p,{children:"When creating URDF for your robot, consider these elements that will be important for later simulation modules:"}),"\n",(0,o.jsx)(n.h3,{id:"1-proper-inertial-properties",children:"1. Proper Inertial Properties"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<inertial>\n  <mass value="1.0"/>\n  <origin xyz="0 0 0"/>\n  <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n</inertial>\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-collision-and-visual-elements",children:"2. Collision and Visual Elements"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<visual>\n  <origin xyz="0 0 0"/>\n  <geometry>\n    <box size="0.5 0.5 0.2"/>\n  </geometry>\n</visual>\n<collision>\n  <origin xyz="0 0 0"/>\n  <geometry>\n    <box size="0.5 0.5 0.2"/>\n  </geometry>\n</collision>\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-transmission-elements-for-actuators",children:"3. Transmission Elements for Actuators"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<transmission name="wheel_trans">\n  <type>transmission_interface/SimpleTransmission</type>\n  <joint name="wheel_joint">\n    <hardwareInterface>hardware_interface/VelocityJointInterface</hardwareInterface>\n  </joint>\n  <actuator name="wheel_motor">\n    <hardwareInterface>hardware_interface/VelocityJointInterface</hardwareInterface>\n    <mechanicalReduction>1</mechanicalReduction>\n  </actuator>\n</transmission>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-example-python-ai-agent-with-urdf-robot",children:"Integration Example: Python AI Agent with URDF Robot"}),"\n",(0,o.jsx)(n.p,{children:"Here's a complete example that combines Python AI with a URDF robot:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\nclass HumanoidController(Node):\n    def __init__(self):\n        super().__init__('humanoid_controller')\n\n        # Publishers for robot commands\n        self.joint_cmd_pub = self.create_publisher(\n            JointState,\n            '/joint_commands',\n            10\n        )\n\n        # Subscribers for sensor data\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Timer for control loop\n        self.control_timer = self.create_timer(0.05, self.control_loop)\n\n        self.joint_positions = {}\n\n    def joint_state_callback(self, msg):\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def control_loop(self):\n        # Simple AI controller logic\n        cmd_msg = JointState()\n        cmd_msg.name = ['left_shoulder_joint', 'right_shoulder_joint']\n        cmd_msg.position = [0.1, -0.1]  # Move arms slightly\n\n        self.joint_cmd_pub.publish(cmd_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = HumanoidController()\n\n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter has shown you how to bridge Python AI agents with physical robot components using rclpy and how to describe robots using URDF. You've learned about:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The rclpy library for Python-ROS 2 integration"}),"\n",(0,o.jsx)(n.li,{children:"How to create AI agents as ROS 2 nodes"}),"\n",(0,o.jsx)(n.li,{children:"The structure of URDF with links, joints, and frames"}),"\n",(0,o.jsx)(n.li,{children:"How to model humanoid robots in URDF"}),"\n",(0,o.jsx)(n.li,{children:"The role of URDF in simulation and real-world deployment"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These concepts form the foundation for connecting your AI algorithms to real robotic systems, preparing you for the simulation, perception, and autonomy modules that will follow."}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Create a simple URDF for a 2-wheeled robot with proper links, joints, and inertial properties."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Write a Python node that subscribes to a camera topic and publishes a command to move the robot forward if no obstacles are detected in the image."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Design a URDF for a simple robotic arm with at least 3 joints and implement a Python controller that moves the arm to a specific position."}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);