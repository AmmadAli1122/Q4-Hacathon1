"use strict";(globalThis.webpackChunkphysical_ai_docs=globalThis.webpackChunkphysical_ai_docs||[]).push([[553],{8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},8995(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"physical-ai/module-4-vla/chapter-2-voice-to-action-language-interfaces","title":"Chapter 2 - Voice-to-Action \u2014 Language Interfaces for Robots","description":"Introduction","source":"@site/docs/physical-ai/module-4-vla/chapter-2-voice-to-action-language-interfaces.md","sourceDirName":"physical-ai/module-4-vla","slug":"/physical-ai/module-4-vla/chapter-2-voice-to-action-language-interfaces","permalink":"/docs/physical-ai/module-4-vla/chapter-2-voice-to-action-language-interfaces","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/module-4-vla/chapter-2-voice-to-action-language-interfaces.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 2 - Voice-to-Action \u2014 Language Interfaces for Robots","sidebar_label":"Chapter 2: Voice-to-Action \u2014 Language Interfaces for Robots"},"sidebar":"physicalAISidebar","previous":{"title":"Chapter 1: Vision\u2013Language\u2013Action \u2014 From Words to Motion","permalink":"/docs/physical-ai/module-4-vla/chapter-1-vision-language-action-words-to-motion"},"next":{"title":"Chapter 3: Capstone \u2014 The Autonomous Humanoid","permalink":"/docs/physical-ai/module-4-vla/chapter-3-capstone-autonomous-humanoid"}}');var o=i(4848),s=i(8453);const r={title:"Chapter 2 - Voice-to-Action \u2014 Language Interfaces for Robots",sidebar_label:"Chapter 2: Voice-to-Action \u2014 Language Interfaces for Robots"},a="Chapter 2: Voice-to-Action \u2014 Language Interfaces for Robots",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Speech-to-Text Using OpenAI Whisper",id:"speech-to-text-using-openai-whisper",level:2},{value:"Introduction to OpenAI Whisper",id:"introduction-to-openai-whisper",level:3},{value:"Whisper Integration for Robotics",id:"whisper-integration-for-robotics",level:3},{value:"Real-time Streaming Approach",id:"real-time-streaming-approach",level:4},{value:"Batch Processing Approach",id:"batch-processing-approach",level:4},{value:"Optimizing Whisper for Robot Environments",id:"optimizing-whisper-for-robot-environments",level:3},{value:"Noise Reduction",id:"noise-reduction",level:4},{value:"Latency Optimization",id:"latency-optimization",level:4},{value:"Translating Voice Commands into Structured Intents",id:"translating-voice-commands-into-structured-intents",level:2},{value:"Intent Recognition Pipeline",id:"intent-recognition-pipeline",level:3},{value:"Natural Language Processing for Robotics",id:"natural-language-processing-for-robotics",level:3},{value:"Command Structure Recognition",id:"command-structure-recognition",level:4},{value:"Named Entity Recognition for Robotics",id:"named-entity-recognition-for-robotics",level:4},{value:"Intent Parser Implementation",id:"intent-parser-implementation",level:3},{value:"Handling Ambiguity and Uncertainty",id:"handling-ambiguity-and-uncertainty",level:3},{value:"Disambiguation Strategies",id:"disambiguation-strategies",level:4},{value:"Example: Resolving &quot;That Object&quot;",id:"example-resolving-that-object",level:4},{value:"Integrating Language Inputs with ROS 2",id:"integrating-language-inputs-with-ros-2",level:2},{value:"ROS 2 Architecture for Language Commands",id:"ros-2-architecture-for-language-commands",level:3},{value:"Language Input Node",id:"language-input-node",level:4},{value:"Command Execution Node",id:"command-execution-node",level:4},{value:"Message Types and Communication Patterns",id:"message-types-and-communication-patterns",level:3},{value:"Custom Message Definitions",id:"custom-message-definitions",level:4},{value:"Topic Architecture",id:"topic-architecture",level:4},{value:"Example Integration Workflow",id:"example-integration-workflow",level:3},{value:"Handling Ambiguity, Confirmation, and User Feedback",id:"handling-ambiguity-confirmation-and-user-feedback",level:2},{value:"Ambiguity Detection and Resolution",id:"ambiguity-detection-and-resolution",level:3},{value:"Types of Ambiguity",id:"types-of-ambiguity",level:4},{value:"Resolution Strategies",id:"resolution-strategies",level:4},{value:"Confirmation Mechanisms",id:"confirmation-mechanisms",level:3},{value:"Explicit Confirmation",id:"explicit-confirmation",level:4},{value:"Implicit Confirmation",id:"implicit-confirmation",level:4},{value:"Designing Robust Human-Robot Interaction Loops",id:"designing-robust-human-robot-interaction-loops",level:3},{value:"Turn-Taking Protocols",id:"turn-taking-protocols",level:4},{value:"Error Recovery Strategies",id:"error-recovery-strategies",level:4},{value:"Practical Implementation Example",id:"practical-implementation-example",level:2},{value:"Complete Voice Command System",id:"complete-voice-command-system",level:3},{value:"Connecting to Previous Modules",id:"connecting-to-previous-modules",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-2-voice-to-action--language-interfaces-for-robots",children:"Chapter 2: Voice-to-Action \u2014 Language Interfaces for Robots"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"This chapter focuses on implementing voice-to-action interfaces that enable robots to understand and respond to natural language commands. We'll explore OpenAI Whisper for speech recognition, techniques for translating voice commands into structured intents, and integration with ROS 2 communication patterns."}),"\n",(0,o.jsx)(n.p,{children:"Building upon the theoretical foundation from Chapter 1, we now move to practical implementation of the language interface. This chapter demonstrates how to create robust human-robot interaction loops that can handle ambiguity, provide confirmation, and offer user feedback."}),"\n",(0,o.jsx)(n.h2,{id:"speech-to-text-using-openai-whisper",children:"Speech-to-Text Using OpenAI Whisper"}),"\n",(0,o.jsx)(n.h3,{id:"introduction-to-openai-whisper",children:"Introduction to OpenAI Whisper"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper is a state-of-the-art automatic speech recognition (ASR) system that excels at transcribing speech in multiple languages. For robotics applications, Whisper offers several advantages:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual Support"}),": Capable of transcribing speech in numerous languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Performs well in various acoustic environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy"}),": High transcription accuracy across different speakers and accents"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Open Source"}),": Available for self-hosting to maintain privacy and reduce latency"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"whisper-integration-for-robotics",children:"Whisper Integration for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"For robotic applications, Whisper can be integrated in several ways:"}),"\n",(0,o.jsx)(n.h4,{id:"real-time-streaming-approach",children:"Real-time Streaming Approach"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\nimport pyaudio\nimport numpy as np\n\nclass RobotWhisperInterface:\n    def __init__(self, model_size="base"):\n        self.model = whisper.load_model(model_size)\n        self.audio_buffer = []\n\n    def process_audio_chunk(self, audio_data):\n        """Process streaming audio and detect speech commands"""\n        self.audio_buffer.extend(audio_data)\n\n        # Process buffer periodically to detect speech\n        if len(self.audio_buffer) > SAMPLE_RATE * 2:  # Process 2-second chunks\n            audio_segment = np.array(self.audio_buffer)\n            result = self.model.transcribe(audio_segment, fp16=False)\n\n            # Reset buffer after processing\n            self.audio_buffer = []\n            return result[\'text\']\n\n        return None\n'})}),"\n",(0,o.jsx)(n.h4,{id:"batch-processing-approach",children:"Batch Processing Approach"}),"\n",(0,o.jsx)(n.p,{children:"For applications where real-time processing isn't critical:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def transcribe_command(audio_file_path):\n    """Transcribe a complete audio command"""\n    model = whisper.load_model("base")\n    result = model.transcribe(audio_file_path)\n    return result[\'text\']\n'})}),"\n",(0,o.jsx)(n.h3,{id:"optimizing-whisper-for-robot-environments",children:"Optimizing Whisper for Robot Environments"}),"\n",(0,o.jsx)(n.p,{children:"Robot environments present unique challenges for speech recognition:"}),"\n",(0,o.jsx)(n.h4,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Acoustic Filtering"}),": Apply noise reduction algorithms before Whisper processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Directional Microphones"}),": Use beamforming microphones to focus on speaker"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Adaptation"}),": Train acoustic models on robot-specific environments"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Selection"}),": Choose Whisper model size based on latency requirements"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hardware Acceleration"}),": Utilize GPU acceleration when available"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Caching"}),": Cache frequently-used phrases for faster recognition"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"translating-voice-commands-into-structured-intents",children:"Translating Voice Commands into Structured Intents"}),"\n",(0,o.jsx)(n.h3,{id:"intent-recognition-pipeline",children:"Intent Recognition Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The process of converting voice commands into structured robotic intents involves several steps:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Voice Input \u2192 Speech-to-Text \u2192 Natural Language Processing \u2192 Intent Extraction \u2192 ROS 2 Commands\n"})}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-processing-for-robotics",children:"Natural Language Processing for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Robotic applications require specialized NLP techniques that differ from general-purpose language understanding:"}),"\n",(0,o.jsx)(n.h4,{id:"command-structure-recognition",children:"Command Structure Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Robotic commands typically follow predictable patterns:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action-Object-Location"}),': "Pick up the red ball from the table"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Commands"}),': "Go to the kitchen" or "Move to the left"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation Commands"}),': "Open the door" or "Pour the water"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Conditional Commands"}),': "If the light is red, stop"']}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"named-entity-recognition-for-robotics",children:"Named Entity Recognition for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Identify key elements in commands:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Objects"}),': "ball", "cup", "door"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Colors"}),': "red", "blue", "green"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Locations"}),': "table", "kitchen", "left side"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Actions"}),': "pick up", "move", "open"']}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"intent-parser-implementation",children:"Intent Parser Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import Dict, List, Optional\n\nclass IntentParser:\n    def __init__(self):\n        self.action_patterns = {\n            'navigation': [\n                r'go to (.+)',\n                r'move to (.+)',\n                r'walk to (.+)',\n                r'go (.+)',\n            ],\n            'manipulation': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'get (.+)',\n                r'take (.+)',\n                r'put (.+) in (.+)',\n                r'place (.+) on (.+)',\n            ],\n            'object_interaction': [\n                r'open (.+)',\n                r'close (.+)',\n                r'turn on (.+)',\n                r'turn off (.+)',\n                r'push (.+)',\n                r'pull (.+)',\n            ]\n        }\n\n    def parse_intent(self, text: str) -> Optional[Dict]:\n        \"\"\"Parse natural language command into structured intent\"\"\"\n        text = text.lower().strip()\n\n        for action_type, patterns in self.action_patterns.items():\n            for pattern in patterns:\n                match = re.match(pattern, text)\n                if match:\n                    groups = match.groups()\n                    return {\n                        'action_type': action_type,\n                        'parameters': list(groups),\n                        'original_text': text\n                    }\n\n        return None\n"})}),"\n",(0,o.jsx)(n.h3,{id:"handling-ambiguity-and-uncertainty",children:"Handling Ambiguity and Uncertainty"}),"\n",(0,o.jsx)(n.p,{children:"Natural language commands often contain ambiguity that must be resolved:"}),"\n",(0,o.jsx)(n.h4,{id:"disambiguation-strategies",children:"Disambiguation Strategies"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contextual Resolution"}),": Use environmental context to resolve ambiguous references"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Clarification Requests"}),": Ask for clarification when multiple interpretations exist"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Confidence Thresholds"}),": Only execute commands with high confidence scores"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"example-resolving-that-object",children:'Example: Resolving "That Object"'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def resolve_deixis(self, command: str, environment_context: dict) -> str:\n    \"\"\"Resolve ambiguous references like 'that object' or 'the red one'\"\"\"\n    if 'that object' in command:\n        # Use object detection to identify the most salient object\n        nearest_object = self.get_nearest_salient_object(environment_context)\n        return command.replace('that object', nearest_object.name)\n\n    return command\n"})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-language-inputs-with-ros-2",children:"Integrating Language Inputs with ROS 2"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-architecture-for-language-commands",children:"ROS 2 Architecture for Language Commands"}),"\n",(0,o.jsx)(n.p,{children:"The integration of language inputs with ROS 2 involves several components:"}),"\n",(0,o.jsx)(n.h4,{id:"language-input-node",children:"Language Input Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\n\nclass LanguageInputNode(Node):\n    def __init__(self):\n        super().__init__('language_input_node')\n        self.subscription = self.create_subscription(\n            String,\n            'voice_commands',\n            self.listener_callback,\n            10)\n        self.command_publisher = self.create_publisher(String, 'parsed_commands', 10)\n        self.intent_parser = IntentParser()\n\n    def listener_callback(self, msg):\n        \"\"\"Process incoming voice command\"\"\"\n        intent = self.intent_parser.parse_intent(msg.data)\n        if intent:\n            # Publish structured command\n            command_msg = String()\n            command_msg.data = str(intent)\n            self.command_publisher.publish(command_msg)\n"})}),"\n",(0,o.jsx)(n.h4,{id:"command-execution-node",children:"Command Execution Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class CommandExecutionNode(Node):\n    def __init__(self):\n        super().__init__('command_execution_node')\n        self.subscription = self.create_subscription(\n            String,\n            'parsed_commands',\n            self.execute_command,\n            10)\n        # Additional publishers for robot actions\n\n    def execute_command(self, msg):\n        \"\"\"Execute parsed command on robot\"\"\"\n        intent = eval(msg.data)  # In practice, use safe deserialization\n        action_type = intent['action_type']\n\n        if action_type == 'navigation':\n            self.execute_navigation(intent['parameters'])\n        elif action_type == 'manipulation':\n            self.execute_manipulation(intent['parameters'])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"message-types-and-communication-patterns",children:"Message Types and Communication Patterns"}),"\n",(0,o.jsx)(n.h4,{id:"custom-message-definitions",children:"Custom Message Definitions"}),"\n",(0,o.jsx)(n.p,{children:"Create custom message types for language commands:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# language_command.msg\nstring action_type\nstring[] parameters\nfloat64 confidence_score\nstring original_command\n"})}),"\n",(0,o.jsx)(n.h4,{id:"topic-architecture",children:"Topic Architecture"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"voice_commands"}),": Raw transcribed text from speech recognition"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"parsed_intents"}),": Structured intents from NLP processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"robot_actions"}),": Low-level commands for robot execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"confirmation_requests"}),": Questions to ask the user for clarification"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-integration-workflow",children:"Example Integration Workflow"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class VLAIntegrator:\n    def __init__(self):\n        self.whisper_interface = RobotWhisperInterface()\n        self.intent_parser = IntentParser()\n        self.ros2_interface = ROS2Interface()\n\n    def process_voice_command(self, audio_input):\n        # Step 1: Transcribe speech to text\n        text = self.whisper_interface.transcribe(audio_input)\n\n        # Step 2: Parse intent from text\n        intent = self.intent_parser.parse_intent(text)\n\n        # Step 3: Validate and resolve ambiguities\n        validated_intent = self.validate_intent(intent)\n\n        # Step 4: Publish to ROS 2\n        self.ros2_interface.publish_command(validated_intent)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"handling-ambiguity-confirmation-and-user-feedback",children:"Handling Ambiguity, Confirmation, and User Feedback"}),"\n",(0,o.jsx)(n.h3,{id:"ambiguity-detection-and-resolution",children:"Ambiguity Detection and Resolution"}),"\n",(0,o.jsx)(n.h4,{id:"types-of-ambiguity",children:"Types of Ambiguity"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Lexical Ambiguity"}),': Words with multiple meanings ("bank" - financial institution vs. river bank)']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Syntactic Ambiguity"}),": Multiple possible grammatical structures"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Referential Ambiguity"}),": Unclear references to objects or locations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scope Ambiguity"}),': Unclear scope of commands ("Turn off all lights before 10 PM")']}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"resolution-strategies",children:"Resolution Strategies"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class AmbiguityResolver:\n    def __init__(self):\n        self.context_resolver = ContextResolver()\n        self.disambiguation_questions = {\n            \'object_reference\': "Do you mean {option1} or {option2}?",\n            \'location_reference\': "Do you mean the {location1} or the {location2}?",\n            \'action_choice\': "Would you like me to {action1} or {action2}?"\n        }\n\n    def resolve_ambiguity(self, intent: Dict) -> Dict:\n        """Resolve ambiguities in the parsed intent"""\n        if self.is_ambiguous(intent):\n            clarification_needed = self.generate_clarification(intent)\n            user_response = self.request_user_input(clarification_needed)\n            return self.update_intent_with_response(intent, user_response)\n\n        return intent\n'})}),"\n",(0,o.jsx)(n.h3,{id:"confirmation-mechanisms",children:"Confirmation Mechanisms"}),"\n",(0,o.jsx)(n.h4,{id:"explicit-confirmation",children:"Explicit Confirmation"}),"\n",(0,o.jsx)(n.p,{children:"For high-risk or complex commands, request explicit confirmation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def request_confirmation(self, intent: Dict) -> bool:\n    """Request user confirmation before executing command"""\n    confirmation_prompt = f"I understood: \'{intent[\'original_text\']}\'. "\n    confirmation_prompt += f"Should I proceed with this action?"\n\n    # Publish to user feedback topic\n    self.publish_feedback_request(confirmation_prompt)\n\n    # Wait for user response\n    user_response = self.wait_for_response(timeout=30.0)\n    return self.parse_affirmative_response(user_response)\n'})}),"\n",(0,o.jsx)(n.h4,{id:"implicit-confirmation",children:"Implicit Confirmation"}),"\n",(0,o.jsx)(n.p,{children:"Provide feedback about the planned action:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def provide_action_feedback(self, intent: Dict):\n    """Provide feedback about the planned action"""\n    feedback_msg = f"Planning to {intent[\'action_type\']} with parameters: {intent[\'parameters\']}"\n    self.publish_system_status(feedback_msg)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"designing-robust-human-robot-interaction-loops",children:"Designing Robust Human-Robot Interaction Loops"}),"\n",(0,o.jsx)(n.h4,{id:"turn-taking-protocols",children:"Turn-Taking Protocols"}),"\n",(0,o.jsx)(n.p,{children:"Establish clear protocols for human-robot interaction:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class InteractionManager:\n    def __init__(self):\n        self.state = 'LISTENING'  # LISTENING, PROCESSING, WAITING_CONFIRMATION, EXECUTING\n        self.timeout_config = {\n            'listening': 10.0,  # seconds to listen for command\n            'processing': 5.0,  # seconds to process command\n            'confirmation': 30.0  # seconds to wait for confirmation\n        }\n\n    def handle_interaction_turn(self, audio_input=None, user_response=None):\n        \"\"\"Handle interaction turn based on current state\"\"\"\n        if self.state == 'LISTENING':\n            if audio_input:\n                self.state = 'PROCESSING'\n                return self.process_command(audio_input)\n\n        elif self.state == 'WAITING_CONFIRMATION':\n            if user_response:\n                self.state = 'EXECUTING'\n                return self.execute_with_response(user_response)\n"})}),"\n",(0,o.jsx)(n.h4,{id:"error-recovery-strategies",children:"Error Recovery Strategies"}),"\n",(0,o.jsx)(n.p,{children:"Handle various types of errors gracefully:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ErrorRecoveryHandler:\n    def handle_recognition_error(self, error_type: str):\n        """Handle speech recognition errors"""\n        if error_type == \'NO_SPEECH_DETECTED\':\n            self.speak("I didn\'t hear anything. Could you repeat your command?")\n        elif error_type == \'LOW_CONFIDENCE\':\n            self.speak("I\'m not sure I understood correctly. Could you repeat that?")\n        elif error_type == \'AMBIGUOUS_COMMAND\':\n            self.handle_ambiguity()\n\n    def handle_execution_error(self, command: Dict, error: Exception):\n        """Handle command execution errors"""\n        self.log_error(error)\n        self.provide_error_feedback(command, error)\n        self.offer_alternatives()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-implementation-example",children:"Practical Implementation Example"}),"\n",(0,o.jsx)(n.h3,{id:"complete-voice-command-system",children:"Complete Voice Command System"}),"\n",(0,o.jsx)(n.p,{children:"Let's implement a complete system that integrates all components:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport numpy as np\n\nclass VoiceCommandRobot(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_robot\')\n\n        # Initialize Whisper model\n        self.whisper_model = whisper.load_model("base")\n\n        # Subscribe to voice commands\n        self.voice_sub = self.create_subscription(\n            String, \'raw_voice_input\', self.voice_callback, 10)\n\n        # Publish parsed commands\n        self.command_pub = self.create_publisher(String, \'parsed_commands\', 10)\n\n        # Initialize intent parser\n        self.intent_parser = IntentParser()\n        self.ambiguity_resolver = AmbiguityResolver()\n\n        # Timer for periodic processing\n        self.timer = self.create_timer(0.1, self.process_pending_commands)\n        self.pending_audio = None\n\n    def voice_callback(self, msg):\n        """Receive raw audio data from microphone"""\n        self.pending_audio = msg.data\n\n    def process_pending_commands(self):\n        """Process pending audio commands"""\n        if self.pending_audio:\n            # Transcribe audio\n            text = self.transcribe_audio(self.pending_audio)\n\n            # Parse intent\n            intent = self.intent_parser.parse_intent(text)\n\n            if intent:\n                # Resolve ambiguities\n                resolved_intent = self.ambiguity_resolver.resolve_ambiguity(intent)\n\n                # Publish command\n                cmd_msg = String()\n                cmd_msg.data = str(resolved_intent)\n                self.command_pub.publish(cmd_msg)\n\n            self.pending_audio = None\n\n    def transcribe_audio(self, audio_data):\n        """Transcribe audio data using Whisper"""\n        # Convert audio data to format expected by Whisper\n        audio_array = np.frombuffer(audio_data, dtype=np.float32)\n        result = self.whisper_model.transcribe(audio_array, fp16=False)\n        return result[\'text\']\n\ndef main(args=None):\n    rclpy.init(args=args)\n    robot = VoiceCommandRobot()\n\n    try:\n        rclpy.spin(robot)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        robot.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"connecting-to-previous-modules",children:"Connecting to Previous Modules"}),"\n",(0,o.jsx)(n.p,{children:"This chapter builds upon concepts from previous modules:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 1 (ROS 2)"}),": Uses ROS 2 communication patterns for language command integration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 2 (Digital Twin)"}),": Simulation environments can be used to test voice command systems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 3 (NVIDIA Isaac)"}),": Perception capabilities support object recognition for ambiguous commands"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter covered the implementation of voice-to-action interfaces using OpenAI Whisper and ROS 2 integration. We explored speech-to-text conversion, intent recognition, ambiguity handling, and robust human-robot interaction loops. The practical implementation examples demonstrate how to create a complete voice command system for robotic applications."}),"\n",(0,o.jsx)(n.p,{children:"The next chapter will focus on the capstone project, integrating all components into a complete autonomous humanoid system."}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a simple Whisper-based speech recognition system that can distinguish between navigation and manipulation commands."}),"\n",(0,o.jsx)(n.li,{children:"Create an intent parser that can handle complex commands with multiple objects and spatial relationships."}),"\n",(0,o.jsx)(n.li,{children:"Design a confirmation mechanism for high-risk commands that ensures user safety."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);